<!DOCTYPE html>
<html lang="zh-TW">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width,initial-scale=1">
    <title>DataLoader 中 pin_memory 的作用 | Python 科學運算筆記</title>
    <meta name="generator" content="VuePress 1.7.1">
    
    <meta name="description" content="Scientific Python Notes">
    
    <link rel="preload" href="/scientific-python-notes/assets/css/0.styles.b062248a.css" as="style"><link rel="preload" href="/scientific-python-notes/assets/js/app.6b22e8a3.js" as="script"><link rel="preload" href="/scientific-python-notes/assets/js/2.7799db7e.js" as="script"><link rel="preload" href="/scientific-python-notes/assets/js/6.74393771.js" as="script"><link rel="prefetch" href="/scientific-python-notes/assets/js/3.489dbf1d.js"><link rel="prefetch" href="/scientific-python-notes/assets/js/4.076001d1.js"><link rel="prefetch" href="/scientific-python-notes/assets/js/5.82e38204.js"><link rel="prefetch" href="/scientific-python-notes/assets/js/7.dbd47d64.js"><link rel="prefetch" href="/scientific-python-notes/assets/js/8.ca060dd8.js">
    <link rel="stylesheet" href="/scientific-python-notes/assets/css/0.styles.b062248a.css">
  </head>
  <body>
    <div id="app" data-server-rendered="true"><div class="theme-container"><header class="navbar"><div class="sidebar-button"><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" role="img" viewBox="0 0 448 512" class="icon"><path fill="currentColor" d="M436 124H12c-6.627 0-12-5.373-12-12V80c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12zm0 160H12c-6.627 0-12-5.373-12-12v-32c0-6.627 5.373-12 12-12h424c6.627 0 12 5.373 12 12v32c0 6.627-5.373 12-12 12z"></path></svg></div> <a href="/scientific-python-notes/" class="home-link router-link-active"><!----> <span class="site-name">Python 科學運算筆記</span></a> <div class="links"><div class="search-box"><input aria-label="Search" autocomplete="off" spellcheck="false" value=""> <!----></div> <nav class="nav-links can-hide"><div class="nav-item"><a href="/scientific-python-notes/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="https://github.com/aquastripe/scientific-python-notes/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav></div></header> <div class="sidebar-mask"></div> <aside class="sidebar"><nav class="nav-links"><div class="nav-item"><a href="/scientific-python-notes/" class="nav-link">
  Home
</a></div><div class="nav-item"><a href="https://github.com/aquastripe/scientific-python-notes/" target="_blank" rel="noopener noreferrer" class="nav-link external">
  Github
  <span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></div> <!----></nav>  <ul class="sidebar-links"><li><a href="/scientific-python-notes/" aria-current="page" class="sidebar-link">前言</a></li><li><section class="sidebar-group depth-0"><p class="sidebar-heading open"><span>PyTorch</span> <!----></p> <ul class="sidebar-links sidebar-group-items"><li><a href="/scientific-python-notes/pytorch/pin_memory.html" aria-current="page" class="active sidebar-link">DataLoader 中 pin_memory 的作用</a></li></ul></section></li></ul> </aside> <main class="page"> <div class="theme-default-content content__default"><h1 id="dataloader-中-pin-memory-的作用"><a href="#dataloader-中-pin-memory-的作用" class="header-anchor">#</a> DataLoader 中 pin_memory 的作用</h1> <p>難度：易</p> <p></p><div class="table-of-contents"><ul><li><a href="#什麼時候該使用">什麼時候該使用</a></li><li><a href="#什麼時候沒有用">什麼時候沒有用</a></li><li><a href="#原理">原理</a></li><li><a href="#如何在-dataloader-中使用">如何在 DataLoader 中使用</a></li><li><a href="#參考資料">參考資料</a></li></ul></div><p></p> <h2 id="什麼時候該使用"><a href="#什麼時候該使用" class="header-anchor">#</a> 什麼時候該使用</h2> <p>在 <code>Dataset</code> 中讀取資料到 CPU (RAM) 上，然後在訓練階段將資料轉移到 GPU 上。</p> <h2 id="什麼時候沒有用"><a href="#什麼時候沒有用" class="header-anchor">#</a> 什麼時候沒有用</h2> <p>在 <code>Dataset</code> 中已經讀取資料到 GPU (vRAM) 上，然後在訓練階段不會將資料轉移到 GPU 上。重點是資料轉移的過程，沒有資料轉移就不會有額外消耗。</p> <h2 id="原理"><a href="#原理" class="header-anchor">#</a> 原理</h2> <p>Pinned 的意思是 page-locked（鎖定分頁的）。在 CPU 中，記憶體預設都是 pageable（可分頁的記憶體）。但是在可分頁記憶體中的資料無法直接傳入 GPU 的記憶體中，傳入前必須先進行鎖定。在 <a href="https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/" target="_blank" rel="noopener noreferrer">NVIDIA blog<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a> 中這麼說：</p> <blockquote><p>Host (CPU) data allocations are pageable by default. The GPU cannot access data directly from pageable host memory, so when a data transfer from pageable host memory to device memory is invoked, the CUDA driver must first allocate a temporary page-locked, or “pinned”, host array, copy the host data to the pinned array, and then transfer the data from the pinned array to device memory, as illustrated below.</p></blockquote> <p><img src="/scientific-python-notes/assets/img/pinned_memory.577c8cbe.jpg" alt=""></p> <p>所以設定 <code>pin_memory=True</code> 就只是先傳入鎖定分頁的記憶體上，之後要搬到 GPU 就可以直接搬。在 <code>DataLoader</code> 中意義更重大，設定好就不會每次讀取資料都要鎖定。</p> <h2 id="如何在-dataloader-中使用"><a href="#如何在-dataloader-中使用" class="header-anchor">#</a> 如何在 DataLoader 中使用</h2> <p>通常是直接下參數 <code>pin_memory=True</code>，它會辨識 <code>Dataset</code> 回傳的物件中，哪些是屬於 <code>Tensor</code> 型別的資料，還有「<code>dict</code> 或是 iterable 的容器物件中，包含 <code>Tensor</code> 型別的資料」，把這些資料傳入 pinned memory。如果是自製的資料型別，就必須自己寫一個包含 <code>pin_memory(self)</code> 函式的類別，傳入參數 <code>collate_fn</code>，來指定設定 <code>pin_memory=True</code> 時該有的行為。以下是官網的範例：</p> <div class="language-python extra-class"><pre class="language-python"><code><span class="token keyword">class</span> <span class="token class-name">SimpleCustomBatch</span><span class="token punctuation">:</span>
    <span class="token keyword">def</span> <span class="token function">__init__</span><span class="token punctuation">(</span>self<span class="token punctuation">,</span> data<span class="token punctuation">)</span><span class="token punctuation">:</span>
        transposed_data <span class="token operator">=</span> <span class="token builtin">list</span><span class="token punctuation">(</span><span class="token builtin">zip</span><span class="token punctuation">(</span><span class="token operator">*</span>data<span class="token punctuation">)</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>inp <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>transposed_data<span class="token punctuation">[</span><span class="token number">0</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>tgt <span class="token operator">=</span> torch<span class="token punctuation">.</span>stack<span class="token punctuation">(</span>transposed_data<span class="token punctuation">[</span><span class="token number">1</span><span class="token punctuation">]</span><span class="token punctuation">,</span> <span class="token number">0</span><span class="token punctuation">)</span>

    <span class="token comment"># custom memory pinning method on custom type</span>
    <span class="token keyword">def</span> <span class="token function">pin_memory</span><span class="token punctuation">(</span>self<span class="token punctuation">)</span><span class="token punctuation">:</span>
        self<span class="token punctuation">.</span>inp <span class="token operator">=</span> self<span class="token punctuation">.</span>inp<span class="token punctuation">.</span>pin_memory<span class="token punctuation">(</span><span class="token punctuation">)</span>
        self<span class="token punctuation">.</span>tgt <span class="token operator">=</span> self<span class="token punctuation">.</span>tgt<span class="token punctuation">.</span>pin_memory<span class="token punctuation">(</span><span class="token punctuation">)</span>
        <span class="token keyword">return</span> self


<span class="token keyword">def</span> <span class="token function">collate_wrapper</span><span class="token punctuation">(</span>batch<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">return</span> SimpleCustomBatch<span class="token punctuation">(</span>batch<span class="token punctuation">)</span>


inps <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">10</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
tgts <span class="token operator">=</span> torch<span class="token punctuation">.</span>arange<span class="token punctuation">(</span><span class="token number">10</span> <span class="token operator">*</span> <span class="token number">5</span><span class="token punctuation">,</span> dtype<span class="token operator">=</span>torch<span class="token punctuation">.</span>float32<span class="token punctuation">)</span><span class="token punctuation">.</span>view<span class="token punctuation">(</span><span class="token number">10</span><span class="token punctuation">,</span> <span class="token number">5</span><span class="token punctuation">)</span>
dataset <span class="token operator">=</span> TensorDataset<span class="token punctuation">(</span>inps<span class="token punctuation">,</span> tgts<span class="token punctuation">)</span>

loader <span class="token operator">=</span> DataLoader<span class="token punctuation">(</span>dataset<span class="token punctuation">,</span> batch_size<span class="token operator">=</span><span class="token number">2</span><span class="token punctuation">,</span> collate_fn<span class="token operator">=</span>collate_wrapper<span class="token punctuation">,</span>
                    pin_memory<span class="token operator">=</span><span class="token boolean">True</span><span class="token punctuation">)</span>

<span class="token keyword">for</span> batch_ndx<span class="token punctuation">,</span> sample <span class="token keyword">in</span> <span class="token builtin">enumerate</span><span class="token punctuation">(</span>loader<span class="token punctuation">)</span><span class="token punctuation">:</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>sample<span class="token punctuation">.</span>inp<span class="token punctuation">.</span>is_pinned<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
    <span class="token keyword">print</span><span class="token punctuation">(</span>sample<span class="token punctuation">.</span>tgt<span class="token punctuation">.</span>is_pinned<span class="token punctuation">(</span><span class="token punctuation">)</span><span class="token punctuation">)</span>
</code></pre></div><p>從範例中可以看到，<code>DataLoader</code> 在取出一個 batch 的資料時，會去呼叫它的 <code>pin_memory()</code> 函式。所以如果自製的 Batch 型別並非上述的容器物件，必須自己實作 <code>pin_memory()</code>。</p> <h2 id="參考資料"><a href="#參考資料" class="header-anchor">#</a> 參考資料</h2> <ul><li><a href="https://pytorch.org/docs/stable/data.html#memory-pinning" target="_blank" rel="noopener noreferrer">https://pytorch.org/docs/stable/data.html#memory-pinning<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-pinning" target="_blank" rel="noopener noreferrer">https://pytorch.org/docs/stable/notes/cuda.html#cuda-memory-pinning<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/" target="_blank" rel="noopener noreferrer">https://developer.nvidia.com/blog/how-optimize-data-transfers-cuda-cc/<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li> <li><a href="https://discuss.pytorch.org/t/when-to-set-pin-memory-to-true/19723" target="_blank" rel="noopener noreferrer">https://discuss.pytorch.org/t/when-to-set-pin-memory-to-true/19723<span><svg xmlns="http://www.w3.org/2000/svg" aria-hidden="true" focusable="false" x="0px" y="0px" viewBox="0 0 100 100" width="15" height="15" class="icon outbound"><path fill="currentColor" d="M18.8,85.1h56l0,0c2.2,0,4-1.8,4-4v-32h-8v28h-48v-48h28v-8h-32l0,0c-2.2,0-4,1.8-4,4v56C14.8,83.3,16.6,85.1,18.8,85.1z"></path> <polygon fill="currentColor" points="45.7,48.7 51.3,54.3 77.2,28.5 77.2,37.2 85.2,37.2 85.2,14.9 62.8,14.9 62.8,22.9 71.5,22.9"></polygon></svg> <span class="sr-only">(opens new window)</span></span></a></li></ul></div> <footer class="page-edit"><!----> <!----></footer> <div class="page-nav"><p class="inner"><span class="prev">
      ←
      <a href="/scientific-python-notes/" class="prev router-link-active">
        前言
      </a></span> <!----></p></div> </main></div><div class="global-ui"></div></div>
    <script src="/scientific-python-notes/assets/js/app.6b22e8a3.js" defer></script><script src="/scientific-python-notes/assets/js/2.7799db7e.js" defer></script><script src="/scientific-python-notes/assets/js/6.74393771.js" defer></script>
  </body>
</html>
